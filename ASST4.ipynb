{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import depthai as dai\n",
    "\n",
    "\n",
    "DIM = (720, 480)\n",
    "\n",
    "extended_disparity = False\n",
    "subpixel = False\n",
    "lr_check = True\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "camRgb = pipeline.create(dai.node.ColorCamera)\n",
    "camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "xoutRgb = pipeline.createXLinkOut()\n",
    "xoutRgb.setStreamName(\"rgb\")\n",
    "camRgb.video.link(xoutRgb.input)\n",
    "confThreshold = 0.5  # Confidence threshold\n",
    "nmsThreshold = 0.4  # Non-maximum suppression threshold\n",
    "inpWidth = 256  # Width of network's input image\n",
    "inpHeight = 256  # Height of network's input image\n",
    "start = time.time()\n",
    "\n",
    "# Load names of classes\n",
    "classesFile = \"/Users/polavarapusaketh/Desktop/asst41/coco.names\"\n",
    "classes = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "# modelConfiguration = \"tiny-yolov2-trial13.cfg\"\n",
    "# modelWeights = \"tiny-yolov2-trial13.weights\"\n",
    "\n",
    "modelConfiguration = \"/Users/polavarapusaketh/Desktop/asst41/tiny.cfg\"\n",
    "modelWeights = \"/Users/polavarapusaketh/Desktop/asst41/yolov3-tiny.weights\"\n",
    "\n",
    "# modelConfiguration = \"yolov3.cfg\"\n",
    "# modelWeights = \"yolov3.weights\"\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    match_mask_color = 255\n",
    "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "# Get the names of the output layers\n",
    "def getOutputsNames(net):\n",
    "    # Get the names of all the layers in the network\n",
    "    layersNames = net.getLayerNames()\n",
    "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
    "    outputlayers = [layersNames[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    return outputlayers\n",
    "\n",
    "\n",
    "# Remove the bounding boxes with low confidence using non-maxima suppression\n",
    "def postprocess(frame, outs):\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "\n",
    "    # Scan through all the bounding boxes output from the network and keep only the\n",
    "    # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
    "    classIds = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            confidence = scores[classId]\n",
    "            if confidence > confThreshold:\n",
    "                center_x = int(detection[0] * frameWidth)\n",
    "                center_y = int(detection[1] * frameHeight)\n",
    "                width = int(detection[2] * frameWidth)\n",
    "                height = int(detection[3] * frameHeight)\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                classIds.append(classId)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "    # lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    # print(indices)\n",
    "    left, top, width, height = 0, 0 , 0, 0\n",
    "    if len(indices) > 0:\n",
    "        for i in indices:\n",
    "            box = boxes[i]\n",
    "            left = box[0]\n",
    "            top = box[1]\n",
    "            width = box[2]\n",
    "            height = box[3]\n",
    "            cv2.putText(frame, classes[classIds[i]], (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, str(round(confidences[i] * 100, 2)) + \"%\", (left, top + height + 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (left, top), (left + width, top + height), (0, 255, 0), 3)\n",
    "\n",
    "\n",
    "\n",
    "# Process inputs\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "counter = 0\n",
    "time_elasped = 0\n",
    "\n",
    "with dai.Device(pipeline) as device:\n",
    "    # Output queue will be used to get the disparity frames from the outputs defined above\n",
    "    qRgb = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    while cv2.waitKey(1) < 0:\n",
    "\n",
    "        # get frame from the video\n",
    "        # hasFrame, frame = cap.read()\n",
    "        inRgb = qRgb.get()\n",
    "        frame = inRgb.getCvFrame()\n",
    "        counter += 1\n",
    "\n",
    "        visualize = frame.copy()\n",
    "\n",
    "        # Create a 4D blob from a frame.\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1 / 255, (inpWidth, inpHeight), [0, 0, 0], 1, crop=False)\n",
    "\n",
    "        # Sets the input to the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Runs the forward pass to get output of the output layers\n",
    "        outs = net.forward(getOutputsNames(net))\n",
    "\n",
    "        # Remove the bounding boxes with low confidence\n",
    "        postprocess(frame, outs)\n",
    "        cv2.putText(frame, \"Q to Exit\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 4)\n",
    "        time_elasped = int(time.time() - start)\n",
    "        if time_elasped > 1:\n",
    "            cv2.putText(frame, \"FPS: \" + str(counter // time_elasped), (50, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2,\n",
    "                        (255, 255, 255), 3)\n",
    "            print(\"FPS: \", counter // time_elasped)\n",
    "\n",
    "        frame = cv2.resize(frame, (720, 480))\n",
    "\n",
    "        cv2.imshow(\"Object Detection YOLO\", frame)\n",
    "\n",
    "        # Stop the program if reached end of video\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            print(\"Done processing !!!\")\n",
    "            # cap.release()\n",
    "            end = time.time()\n",
    "            print(\"Time Elasped: \", int(end - start))\n",
    "            print(\"FPS: \", counter // (end - start))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install depthai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
